#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Veri Toplama Ä°ÅŸlemleri

Bu modÃ¼l, ham akÄ±llÄ± sayaÃ§ verilerini sistem seviyesinde toplar ve
gÃ¼nlÃ¼k toplam enerji tÃ¼ketimi hesaplar.
"""

import time
from pyspark.sql.functions import (
    col, sum as spark_sum, avg as spark_avg, max as spark_max, min as spark_min,
    count as spark_count, date_format, hour, dayofweek, month, year
)
from pyspark.sql.types import TimestampType

from utils.logger import get_logger
from utils.config import PREPROCESSING_CONFIG
from utils.spark_utils import cache_dataframe, show_dataframe_info

# Logger oluÅŸtur
logger = get_logger(__name__)

def create_system_level_aggregation(df):
    """
    MÃ¼ÅŸteri verilerini sistem seviyesine topla (zaman damgasÄ± baÅŸÄ±na)
    
    Args:
        df: Ham veri DataFrame'i
        
    Returns:
        tuple: (system_df, processing_time)
    """
    logger.info("ğŸ”§ SÄ°STEM SEVÄ°YESÄ° TOPLAMA OLUÅTURULUYOR...")
    start_time = time.time()
    
    # Zaman damgasÄ±na gÃ¶re grupla ve tÃ¼m mÃ¼ÅŸterileri topla
    system_df = df.groupBy("full_timestamp") \
        .agg(
            spark_sum("load_percentage").alias("total_system_load"),
            spark_avg("load_percentage").alias("avg_system_load"),
            spark_max("load_percentage").alias("max_system_load"),
            spark_min("load_percentage").alias("min_system_load"),
            spark_count("customer_id").alias("active_customers")
        )
    
    # Zaman bazlÄ± Ã¶zellikler ekle
    system_df = system_df \
        .withColumn("date", date_format("full_timestamp", "yyyy-MM-dd")) \
        .withColumn("hour", hour("full_timestamp")) \
        .withColumn("month", month("full_timestamp")) \
        .withColumn("year", year("full_timestamp")) \
        .withColumn("dayofweek", dayofweek("full_timestamp"))
    
    # Ä°statistikleri hesaplamadan Ã¶nce Ã¶nbelleÄŸe al
    system_df = cache_dataframe(system_df, "Sistem Seviyesi Veri")
    
    # Ä°ÅŸleme sÃ¼resini ve kayÄ±t sayÄ±sÄ±nÄ± hesapla
    processing_time = time.time() - start_time
    record_count = system_df.count()
    
    # Ä°ÅŸleme metrikleri
    logger.info(f"âœ… Sistem toplama iÅŸlemi {processing_time:.1f}s sÃ¼rede tamamlandÄ±")
    logger.info(f"ğŸ“Š Sistem kayÄ±tlarÄ±: {record_count:,} zaman damgasÄ±")
    
    # Verilerin daÄŸÄ±lÄ±mÄ±nÄ± analiz et
    load_stats = system_df.select(
        spark_avg("total_system_load").alias("avg_load"),
        spark_min("total_system_load").alias("min_load"),
        spark_max("total_system_load").alias("max_load"),
        spark_avg("active_customers").alias("avg_customers")
    ).collect()[0]
    
    logger.info(f"ğŸ“ˆ SÄ°STEM YÃœKÃœ Ä°STATÄ°STÄ°KLERÄ°:")
    logger.info(f"   Ortalama yÃ¼k: {load_stats['avg_load']:.2f}")
    logger.info(f"   Minimum yÃ¼k: {load_stats['min_load']:.2f}")
    logger.info(f"   Maksimum yÃ¼k: {load_stats['max_load']:.2f}")
    logger.info(f"   Ortalama aktif mÃ¼ÅŸteri: {load_stats['avg_customers']:.1f}")
    
    # Ã–rnek veri gÃ¶ster
    logger.info("ğŸ“‹ Ã–rnek sistem verisi:")
    system_df.select("full_timestamp", "total_system_load", "avg_system_load", "active_customers").show(5)
    
    return system_df, processing_time

def create_daily_total_energy(system_df):
    """
    GÃ¼nlÃ¼k toplam enerji tÃ¼ketimi hesapla (gÃ¼n baÅŸÄ±na tÃ¼m zaman damgalarÄ±nÄ± topla)
    
    Args:
        system_df: Sistem seviyesi DataFrame
        
    Returns:
        tuple: (daily_df, day_count, processing_time)
    """
    logger.info("ğŸ“Š GÃœNLÃœK TOPLAM ENERJÄ° OLUÅTURULUYOR...")
    start_time = time.time()
    
    # Tarihe gÃ¶re grupla ve o gÃ¼n iÃ§in tÃ¼m zaman damgalarÄ±nÄ± topla
    daily_df = system_df.groupBy("date") \
        .agg(
            spark_sum("total_system_load").alias("total_daily_energy"),  # Ana hedef!
            spark_avg("total_system_load").alias("avg_daily_load"),
            spark_max("total_system_load").alias("peak_daily_load"),
            spark_min("total_system_load").alias("min_daily_load"),
            spark_sum("active_customers").alias("total_daily_customers"),
            spark_avg("active_customers").alias("avg_daily_customers")
        )
    
    # Tarih Ã¶zellikleri ekle
    daily_df = daily_df \
        .withColumn("timestamp", col("date").cast(TimestampType())) \
        .withColumn("month", month(col("timestamp"))) \
        .withColumn("year", year(col("timestamp"))) \
        .withColumn("dayofweek", dayofweek(col("timestamp")))
    
    # Tarihe gÃ¶re sÄ±rala
    daily_df = daily_df.orderBy("date")
    
    # Yeniden bÃ¶lÃ¼mle ve Ã¶nbelleÄŸe al (daha iyi performans iÃ§in)
    if "repartition_count" in PREPROCESSING_CONFIG:
        partition_count = PREPROCESSING_CONFIG["repartition_count"]
        logger.info(f"ğŸ”„ GÃ¼nlÃ¼k veri {partition_count} partitiona yeniden bÃ¶lÃ¼mleniyor...")
        daily_df = daily_df.repartition(partition_count)
    
    # Ã–nbelleÄŸe al
    daily_df = cache_dataframe(daily_df, "GÃ¼nlÃ¼k Toplam Enerji")
    
    # Ä°ÅŸlem sÃ¼resini ve gÃ¼n sayÄ±sÄ±nÄ± hesapla
    processing_time = time.time() - start_time
    day_count = daily_df.count()
    
    logger.info(f"âœ… GÃ¼nlÃ¼k toplama iÅŸlemi {processing_time:.1f}s sÃ¼rede tamamlandÄ±")
    logger.info(f"ğŸ“Š Toplam gÃ¼n: {day_count}")
    
    # GÃ¼nlÃ¼k veriler hakkÄ±nda detaylÄ± istatistikler
    logger.info("ğŸ“‹ Ã–rnek gÃ¼nlÃ¼k veri:")
    daily_df.select("date", "total_daily_energy", "avg_daily_load", "peak_daily_load").show(10)
    
    # Enerji istatistiklerini gÃ¶ster
    energy_stats = daily_df.select(
        spark_avg("total_daily_energy").alias("avg_daily_energy"),
        spark_min("total_daily_energy").alias("min_daily_energy"),
        spark_max("total_daily_energy").alias("max_daily_energy"),
        (spark_max("total_daily_energy") / spark_min("total_daily_energy")).alias("max_min_ratio")
    ).collect()[0]
    
    logger.info(f"ğŸ“ˆ GÃœNLÃœK ENERJÄ° Ä°STATÄ°STÄ°KLERÄ°:")
    logger.info(f"   Ortalama: {energy_stats['avg_daily_energy']:,.1f}")
    logger.info(f"   Minimum: {energy_stats['min_daily_energy']:,.1f}")
    logger.info(f"   Maksimum: {energy_stats['max_daily_energy']:,.1f}")
    logger.info(f"   Maks/Min oranÄ±: {energy_stats['max_min_ratio']:,.2f}")
    
    # AylÄ±k Ã¶zete gÃ¶re toplam enerjiyi gÃ¶ster
    logger.info("ğŸ“… AYLIK ENERJÄ° DAÄILIMI:")
    monthly_stats = daily_df.groupBy("month") \
        .agg(
            spark_sum("total_daily_energy").alias("monthly_energy"),
            spark_avg("total_daily_energy").alias("avg_daily_energy"),
            spark_count("date").alias("day_count")
        ) \
        .orderBy("month")
    
    monthly_stats.show()
    
    return daily_df, day_count, processing_time