#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ã–zellik MÃ¼hendisliÄŸi ModÃ¼lÃ¼

Bu modÃ¼l, zaman serisi verisi iÃ§in ileri dÃ¼zey Ã¶zellikler oluÅŸturur ve
ML modellemesi iÃ§in veri setini hazÄ±rlar.
"""

import time
from pyspark.sql.functions import (
    col, lag, sin, cos, when, avg as spark_avg, max as spark_max, min as spark_min
)
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

from utils.logger import get_logger
from utils.config import PREPROCESSING_CONFIG, MODEL_CONFIG
from utils.spark_utils import cache_dataframe, show_dataframe_info

# Logger oluÅŸtur
logger = get_logger(__name__)

def create_advanced_features(daily_df):
    """
    Gecikmeli Ã¶zellikler, hareketli ortalamalar ve zaman Ã¶zellikleri oluÅŸtur
    
    Args:
        daily_df: GÃ¼nlÃ¼k toplam enerji DataFrame'i
        
    Returns:
        tuple: (feature_df, feature_columns, processing_time)
    """
    logger.info("ğŸ§  GELÄ°ÅMÄ°Å Ã–ZELLÄ°KLER OLUÅTURULUYOR...")
    start_time = time.time()
    
    # Zaman penceresi tanÄ±mla (tarih sÄ±rasÄ±na gÃ¶re)
    window_spec = Window.orderBy("date")
    
    # 1. Gecikmeli Ã¶zellikler (Ã¶nceki gÃ¼nler)
    logger.info("â±ï¸  Gecikmeli Ã¶zellikler oluÅŸturuluyor...")
    daily_df = daily_df \
        .withColumn("energy_lag_1", lag("total_daily_energy", 1).over(window_spec)) \
        .withColumn("energy_lag_2", lag("total_daily_energy", 2).over(window_spec)) \
        .withColumn("energy_lag_3", lag("total_daily_energy", 3).over(window_spec)) \
        .withColumn("energy_lag_7", lag("total_daily_energy", 7).over(window_spec))
    
    # 2. Hareketli ortalamalar (pencere aÄŸÄ±rlÄ±klarÄ±)
    logger.info("ğŸ“Š Hareketli ortalama Ã¶zellikleri oluÅŸturuluyor...")
    rolling_window_3 = Window.orderBy("date").rowsBetween(-2, 0)  # Son 3 gÃ¼n
    rolling_window_7 = Window.orderBy("date").rowsBetween(-6, 0)  # Son 7 gÃ¼n
    rolling_window_30 = Window.orderBy("date").rowsBetween(-29, 0)  # Son 30 gÃ¼n
    
    daily_df = daily_df \
        .withColumn("rolling_avg_3d", spark_avg("total_daily_energy").over(rolling_window_3)) \
        .withColumn("rolling_avg_7d", spark_avg("total_daily_energy").over(rolling_window_7)) \
        .withColumn("rolling_avg_30d", spark_avg("total_daily_energy").over(rolling_window_30)) \
        .withColumn("rolling_max_7d", spark_max("total_daily_energy").over(rolling_window_7)) \
        .withColumn("rolling_min_7d", spark_min("total_daily_energy").over(rolling_window_7))
    
    # 3. DÃ¶ngÃ¼sel zaman Ã¶zellikleri (sinÃ¼s/kosinÃ¼s kodlama)
    logger.info("ğŸ”„ DÃ¶ngÃ¼sel zaman Ã¶zellikleri oluÅŸturuluyor...")
    daily_df = daily_df \
        .withColumn("month_sin", sin(col("month") * 2 * 3.14159 / 12)) \
        .withColumn("month_cos", cos(col("month") * 2 * 3.14159 / 12)) \
        .withColumn("dayofweek_sin", sin(col("dayofweek") * 2 * 3.14159 / 7)) \
        .withColumn("dayofweek_cos", cos(col("dayofweek") * 2 * 3.14159 / 7))
    
    # 4. TÃ¼retilmiÅŸ Ã¶zellikler (kategorik ve mevsimsel)
    logger.info("ğŸŒ Mevsimsel ve kategorik Ã¶zellikler oluÅŸturuluyor...")
    daily_df = daily_df \
        .withColumn("is_weekend", when(col("dayofweek").isin([1, 7]), 1).otherwise(0)) \
        .withColumn("is_summer", when(col("month").isin([12, 1, 2]), 1).otherwise(0)) \
        .withColumn("is_winter", when(col("month").isin([6, 7, 8]), 1).otherwise(0)) \
        .withColumn("peak_to_avg_ratio", col("peak_daily_load") / col("avg_daily_load"))
    
    # 5. EÄŸilim (trend) Ã¶zellikleri
    logger.info("ğŸ“ˆ EÄŸilim Ã¶zellikleri oluÅŸturuluyor...")
    daily_df = daily_df \
        .withColumn("energy_trend_3d", col("total_daily_energy") - col("rolling_avg_3d")) \
        .withColumn("energy_trend_7d", col("total_daily_energy") - col("rolling_avg_7d"))
    
    # Ã–nbelleÄŸe al
    feature_df = cache_dataframe(daily_df, "GeliÅŸmiÅŸ Ã–zellikler DataFrame")
    
    # Ä°ÅŸleme sÃ¼resini hesapla
    processing_time = time.time() - start_time
    
    # Ã–zellik listesini oluÅŸtur (hedef deÄŸiÅŸken hariÃ§)
    feature_columns = [c for c in feature_df.columns if c not in ['date', 'timestamp', 'total_daily_energy']]
    
    # Ã–zellik Ã¶zetleri
    logger.info(f"âœ… GeliÅŸmiÅŸ Ã¶zellikler {processing_time:.1f}s sÃ¼rede oluÅŸturuldu")
    logger.info(f"ğŸ“Š Toplam Ã¶zellik sayÄ±sÄ±: {len(feature_columns)}")
    
    # Ã–zellik kategorilerine gÃ¶re grupla ve logla
    logger.info("ğŸ§© Ã–ZELLÄ°K KATEGORÄ°LERÄ°:")
    
    # Gecikmeli Ã¶zellikler
    lag_features = [f for f in feature_columns if 'lag' in f]
    logger.info(f"   â±ï¸  Gecikmeli Ã¶zellikler ({len(lag_features)}): {', '.join(lag_features)}")
    
    # Hareketli ortalama Ã¶zellikleri
    rolling_features = [f for f in feature_columns if 'rolling' in f]
    logger.info(f"   ğŸ“Š Hareketli ortalama Ã¶zellikleri ({len(rolling_features)}): {', '.join(rolling_features)}")
    
    # Zaman Ã¶zellikleri
    time_features = [f for f in feature_columns if any(x in f for x in ['sin', 'cos', 'month', 'day'])]
    logger.info(f"   ğŸ”„ Zaman Ã¶zellikleri ({len(time_features)}): {', '.join(time_features)}")
    
    # Kategorik Ã¶zellikler
    categorical_features = [f for f in feature_columns if f.startswith('is_')]
    logger.info(f"   ğŸ·ï¸  Kategorik Ã¶zellikler ({len(categorical_features)}): {', '.join(categorical_features)}")
    
    # EÄŸilim Ã¶zellikleri
    trend_features = [f for f in feature_columns if 'trend' in f]
    logger.info(f"   ğŸ“ˆ EÄŸilim Ã¶zellikleri ({len(trend_features)}): {', '.join(trend_features)}")
    
    # DiÄŸer Ã¶zellikler
    other_features = [f for f in feature_columns if f not in lag_features + rolling_features + 
                      time_features + categorical_features + trend_features]
    if other_features:
        logger.info(f"   ğŸ”§ DiÄŸer Ã¶zellikler ({len(other_features)}): {', '.join(other_features)}")
    
    return feature_df, feature_columns, processing_time

def prepare_ml_dataset(daily_df, feature_columns, test_days=None):
    """
    EÄŸitim/doÄŸrulama/test bÃ¶lÃ¼mleri ile ML veri seti hazÄ±rla
    
    Args:
        daily_df: Ã–zellik eklenmiÅŸ DataFrame
        feature_columns: Ã–zellik kolonu isimleri
        test_days: Test iÃ§in ayrÄ±lacak gÃ¼n sayÄ±sÄ± (None ise config'den alÄ±nÄ±r)
        
    Returns:
        tuple: (train_df, val_df, test_df, train_count, val_count, test_count)
    """
    # Test gÃ¼n sayÄ±sÄ±nÄ± config'den al (belirtilmemiÅŸse)
    if test_days is None:
        test_days = MODEL_CONFIG.get("test_days", 60)
    
    logger.info(f"ğŸ¯ ML VERÄ° SETÄ° HAZIRLANIYOR...")
    logger.info(f"ğŸ“ Test periyodu: Son {test_days} gÃ¼n")
    
    # Null deÄŸerleri temizle (gecikmeli Ã¶zelliklerden kaynaklÄ±)
    clean_df = daily_df.dropna()
    
    total_rows = clean_df.count()
    logger.info(f"ğŸ“Š Temiz veri seti: {total_rows} gÃ¼n")
    
    # Veri yeterli mi kontrol et
    if total_rows < test_days + 30:
        logger.warning(f"âš ï¸  UYARI: Yeterli veri yok! Sadece {total_rows} gÃ¼n mevcut")
        test_days = max(10, total_rows // 4)
        logger.warning(f"ğŸ”„ Test gÃ¼nleri ÅŸu ÅŸekilde ayarlandÄ±: {test_days}")
    
    # SÄ±ralama iÃ§in satÄ±r numarasÄ± ekle
    window_spec = Window.orderBy("date")
    indexed_df = clean_df.withColumn("row_num", row_number().over(window_spec))
    
    # Veri setini bÃ¶l
    train_val_ratio = PREPROCESSING_CONFIG.get("train_val_test_split", [0.7, 0.15, 0.15])
    val_size = 15  # DoÄŸrulama iÃ§in 15 gÃ¼n rezerve et
    
    train_size = total_rows - test_days - val_size
    
    train_df = indexed_df.filter(col("row_num") <= train_size)
    val_df = indexed_df.filter((col("row_num") > train_size) & (col("row_num") <= train_size + val_size))
    test_df = indexed_df.filter(col("row_num") > train_size + val_size)
    
    # Set bÃ¼yÃ¼klÃ¼klerini hesapla
    train_count = train_df.count()
    val_count = val_df.count()
    test_count = test_df.count()
    
    # Veri bÃ¶lÃ¼nmesi hakkÄ±nda bilgi
    logger.info(f"âœ… VERÄ° BÃ–LÃœNMESÄ°:")
    logger.info(f"   ğŸ‹ï¸  EÄŸitim: {train_count} gÃ¼n ({train_count/total_rows*100:.1f}%)")
    logger.info(f"   ğŸ” DoÄŸrulama: {val_count} gÃ¼n ({val_count/total_rows*100:.1f}%)")
    logger.info(f"   ğŸ§ª Test: {test_count} gÃ¼n ({test_count/total_rows*100:.1f}%)")
    
    # Tarih aralÄ±klarÄ±nÄ± gÃ¶ster - âœ… DÃœZELTME: spark_min ve spark_max kullan
    train_dates = train_df.select(spark_min("date").alias("start"), spark_max("date").alias("end")).collect()[0]
    val_dates = val_df.select(spark_min("date").alias("start"), spark_max("date").alias("end")).collect()[0]
    test_dates = test_df.select(spark_min("date").alias("start"), spark_max("date").alias("end")).collect()[0]
    
    logger.info(f"   ğŸ“… EÄŸitim tarihleri: {train_dates['start']} - {train_dates['end']}")
    logger.info(f"   ğŸ“… DoÄŸrulama tarihleri: {val_dates['start']} - {val_dates['end']}")
    logger.info(f"   ğŸ“… Test tarihleri: {test_dates['start']} - {test_dates['end']}")
    
    # Ã–zellik istatistikleri
    logger.info(f"   ğŸ§© Toplam Ã¶zellik sayÄ±sÄ±: {len(feature_columns)}")
    
    return train_df, val_df, test_df, train_count, val_count, test_count