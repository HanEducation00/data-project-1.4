#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Veri YÃ¼kleme ModÃ¼lÃ¼

Bu modÃ¼l akÄ±llÄ± sayaÃ§ verilerini yerel dosyalardan yÃ¼kler.
"""

import os
from pyspark.sql.functions import col, regexp_extract, to_timestamp
from pyspark.sql.types import StringType, DoubleType, TimestampType

from utils.logger import get_logger
from utils.config import DATA_CONFIG

from pyspark.sql.types import ArrayType, StructType, StructField, StringType, TimestampType, FloatType, IntegerType
# Logger oluÅŸtur
logger = get_logger(__name__)

def load_smart_meter_data(spark):
    """
    AkÄ±llÄ± sayaÃ§ verilerini yerel dosyalardan yÃ¼kle
    
    Args:
        spark: Spark session
        
    Returns:
        tuple: (DataFrame, record_count)
    """
    logger.info("ğŸ“ AKILLI SAYAÃ‡ VERÄ°LERÄ° YÃœKLENÄ°YOR...")
    
    # Ana veri yolu
    base_path = DATA_CONFIG["base_path"]
    file_pattern = DATA_CONFIG["file_pattern"]
    data_path = f"{base_path}/{file_pattern}"
    
    logger.info(f"ğŸ” Ana yol kontrol ediliyor: {base_path}")
    
    # Ana yol var mÄ± kontrol et
    if os.path.exists(base_path) and len(os.listdir(base_path)) > 0:
        logger.info(f"âœ… Yerel akÄ±llÄ± sayaÃ§ dosyalarÄ± bulundu: {base_path}")
        return load_local_files(spark, data_path)
    else:
        # Alternatif yollarÄ± dene
        for alt_path in DATA_CONFIG["alt_paths"]:
            logger.info(f"ğŸ” Alternatif yol kontrol ediliyor: {alt_path}")
            
            if os.path.exists(alt_path) and len(os.listdir(alt_path)) > 0:
                alt_data_path = f"{alt_path}/{file_pattern}"
                logger.info(f"âœ… Alternatif yolda dosyalar bulundu: {alt_path}")
                return load_local_files(spark, alt_data_path)
        
        # HiÃ§bir yerde dosya bulunamadÄ±
        logger.error("âŒ HiÃ§bir veri dosyasÄ± bulunamadÄ±!")
        raise FileNotFoundError("Veri dosyalarÄ± bulunamadÄ±. LÃ¼tfen veri yollarÄ±nÄ± kontrol edin.")

def load_local_files(spark, data_path):
    """
    GerÃ§ek akÄ±llÄ± sayaÃ§ txt dosyalarÄ±nÄ± yÃ¼kle
    
    Args:
        spark: Spark session
        data_path: Dosya yolu pattern
        
    Returns:
        tuple: (DataFrame, record_count)
    """
    logger.info(f"ğŸ“‚ YÃ¼kleniyor: {data_path}")
    
    try:
        # Ã–nce bir dosyayÄ± Ã¶rnek olarak inceleyip yapÄ±sÄ±nÄ± anla
        sample_files = spark.sparkContext.wholeTextFiles(data_path, 1).take(1)
        
        if not sample_files:
            logger.error(f"âŒ Veri yolunda dosya bulunamadÄ±: {data_path}")
            raise FileNotFoundError(f"Belirtilen yolda dosya bulunamadÄ±: {data_path}")
        
        # Ã–rnek dosyanÄ±n iÃ§eriÄŸini incele
        sample_content = sample_files[0][1]
        logger.info(f"ğŸ“ Ã–rnek dosya inceleniyor...")
        
        # Dosya formatÄ±nÄ± algÄ±la
        if "FORMAT=" in sample_content:
            logger.info("âœ… Ã–zel formatlÄ± veri dosyasÄ± tespit edildi (FORMAT= iÃ§eriyor)")
            # DosyalarÄ± Ã¶zel formatta oku - FORMAT= iÃ§eren dosyalar
            return load_formatted_files(spark, data_path)
        else:
            # Standart CSV olarak oku
            logger.info("ğŸ“„ Standart CSV formatÄ±nda okuma deneniyor...")
            return load_csv_files(spark, data_path)
    
    except Exception as e:
        logger.error(f"âŒ Yerel dosyalarÄ± yÃ¼kleme hatasÄ±: {e}")
        raise RuntimeError(f"Veri yÃ¼kleme hatasÄ±: {e}")

def load_formatted_files(spark, data_path):
    """
    Ã–zel formatlÄ± dosyalarÄ± yÃ¼kle (FORMAT= iÃ§eren dosyalar)
    
    Args:
        spark: Spark session
        data_path: Dosya yolu pattern
        
    Returns:
        tuple: (DataFrame, record_count)
    """
    logger.info("ğŸ“Š Ã–zel formatlÄ± dosyalarÄ± iÅŸleme...")
    
    try:
        # DosyalarÄ± metin olarak oku
        raw_df = spark.read.text(data_path)
        
        # Veri satÄ±rlarÄ±nÄ± filtrele (res_ ile baÅŸlayanlar)
        data_lines = raw_df.filter(col("value").startswith("res_"))
        
        # Ã–rnek veri satÄ±rÄ±nÄ± al
        sample_line = data_lines.first()
        if sample_line:
            logger.info(f"ğŸ“‹ Ã–rnek veri satÄ±rÄ±: {sample_line['value'][:100]}...")
        
        # Explode iÅŸlemi iÃ§in yardÄ±mcÄ± fonksiyon
        def parse_and_explode(line):
            parts = line.split(',')
            if len(parts) < 12:  # En az 11 meta veri alanÄ± + 1 deÄŸer olmalÄ±
                return []
                
            customer_id = parts[0].replace("res_", "")
            profile_type = parts[1]
            year = parts[6]
            month_str = parts[7]
            day = parts[8]
            
            # Ay deÄŸerini sayÄ±sal deÄŸere dÃ¶nÃ¼ÅŸtÃ¼r
            month_map = {
                "JANUARY": 1, "FEBRUARY": 2, "MARCH": 3, "APRIL": 4,
                "MAY": 5, "JUNE": 6, "JULY": 7, "AUGUST": 8,
                "SEPTEMBER": 9, "OCTOBER": 10, "NOVEMBER": 11, "DECEMBER": 12
            }
            month = month_map.get(month_str, 1)
            
            # YÃ¼k deÄŸerlerini al (12. sÃ¼tundan itibaren)
            load_values = parts[11:]
            
            # Her deÄŸer iÃ§in satÄ±r oluÅŸtur
            result_rows = []
            for i, load_value in enumerate(load_values):
                if not load_value or load_value.upper() in ['NONE', 'NULL']:
                    continue
                
                # 15 dakikalÄ±k aralÄ±klarÄ± saat ve dakikaya dÃ¶nÃ¼ÅŸtÃ¼r
                hour = (i * 15) // 60
                minute = (i * 15) % 60
                
                # Zaman damgasÄ± oluÅŸtur
                from datetime import datetime
                timestamp = datetime(int(year), month, int(day), hour, minute)
                
                try:
                    # YÃ¼k deÄŸerini float'a dÃ¶nÃ¼ÅŸtÃ¼r
                    load_pct = float(load_value)
                    
                    # SatÄ±r oluÅŸtur
                    result_rows.append({
                        'customer_id': customer_id,
                        'profile_type': profile_type,
                        'full_timestamp': timestamp,
                        'load_percentage': load_pct,
                        'day_num': int(day),
                        'hour': hour,
                        'minute': minute,
                        'month': month,
                        'year': int(year)
                    })
                except ValueError:
                    # SayÄ±sal olmayan deÄŸerleri atla
                    pass
                    
            return result_rows
            

        # Ã‡Ä±ktÄ± ÅŸemasÄ± tanÄ±mla
        output_schema = ArrayType(StructType([
            StructField("customer_id", StringType(), True),
            StructField("profile_type", StringType(), True),
            StructField("full_timestamp", TimestampType(), True),
            StructField("load_percentage", FloatType(), True),
            StructField("day_num", IntegerType(), True),
            StructField("hour", IntegerType(), True),
            StructField("minute", IntegerType(), True),
            StructField("month", IntegerType(), True),
            StructField("year", IntegerType(), True)
        ]))
        
        # UDF'i kaydet
        from pyspark.sql.functions import udf, explode
        parse_udf = udf(parse_and_explode, output_schema)
        
        # SatÄ±rlarÄ± parse et ve explode ile her deÄŸeri ayrÄ± satÄ±ra Ã§Ä±kar
        parsed_df = data_lines.select(
            explode(parse_udf(col("value"))).alias("parsed")
        ).select("parsed.*")
        
        # Ã–nbelleÄŸe al
        parsed_df.cache()
        count = parsed_df.count()
        
        logger.info(f"âœ… Ã–zel format verisi yÃ¼klendi: {count:,} kayÄ±t")
        
        # Ã–rnek veri
        logger.info("ğŸ“‹ Ã–rnek veri:")
        parsed_df.show(5, truncate=False)
        
        return parsed_df, count
        
    except Exception as e:
        logger.error(f"âŒ Ã–zel format verisi yÃ¼kleme hatasÄ±: {e}")
        import traceback
        logger.debug(f"Hata detaylarÄ±:\n{traceback.format_exc()}")
        raise RuntimeError(f"Ã–zel format verisi yÃ¼kleme hatasÄ±: {e}")

def load_csv_files(spark, data_path):
    """
    Standart CSV formatÄ±nda dosyalarÄ± yÃ¼kle
    
    Args:
        spark: Spark session
        data_path: Dosya yolu pattern
        
    Returns:
        tuple: (DataFrame, record_count)
    """
    logger.info("ğŸ“„ Standart CSV formatÄ±nda yÃ¼kleme deneniyor...")
    
    try:
        # Ã–nce ham CSV'yi yÃ¼kle
        raw_df = spark.read.csv(data_path, header=True, inferSchema=True)
        
        # KolonlarÄ± gÃ¶ster
        logger.info(f"ğŸ“‹ CSV kolon isimleri: {raw_df.columns}")
        
        # Ã–rnek veri
        logger.info("ğŸ“‹ Ã–rnek ham veri:")
        raw_df.show(5, truncate=False)
        
        # Mevcut kolon isimlerine gÃ¶re dÃ¶nÃ¼ÅŸÃ¼m yap
        # NOT: GerÃ§ek kolon isimlerinize gÃ¶re bu kÄ±smÄ± dÃ¼zenleyin
        
        df = raw_df
        
        # Ã–nbelleÄŸe al
        df.cache()
        count = df.count()
        
        logger.info(f"âœ… CSV verisi yÃ¼klendi: {count:,} kayÄ±t")
        
        return df, count
        
    except Exception as e:
        logger.error(f"âŒ CSV verisi yÃ¼kleme hatasÄ±: {e}")
        raise RuntimeError(f"CSV verisi yÃ¼kleme hatasÄ±: {e}")