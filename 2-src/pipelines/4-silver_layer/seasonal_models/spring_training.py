#!/usr/bin/env python3
"""
Bahar Sezonu (Ocak-MayÄ±s) ML Model EÄŸitimi
Spring Season Energy Consumption Model Training
Spark Client'te Local Mode ile Ã‡alÄ±ÅŸacak Versiyon
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from pyspark.sql import SparkSession
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import max as spark_max, col, avg, min as spark_min, count
from datetime import datetime

import mlflow
import mlflow.spark

# ğŸ¯ TRACKING URI EKLEME - SUMMER GÄ°BÄ°!
mlflow.set_tracking_uri("http://mlflow-server:5000")

# âœ… BRONZ LAYER GÄ°BÄ° LOCAL MODE - TÃœM CORE'LARI KULLAN
spark = SparkSession.builder \
    .appName("Spring Season Model Training") \
    .master("local[*]") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.7.2") \
    .config("spark.driver.maxResultSize", "2g") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# Loglama seviyesini ayarla
spark.sparkContext.setLogLevel("WARN")

# PostgreSQL baÄŸlantÄ± bilgileri
jdbc_url = "jdbc:postgresql://development-postgres:5432/datawarehouse"
connection_properties = {
    "user": "datauser",
    "password": "datapass",
    "driver": "org.postgresql.Driver"
}

def check_data_availability(target_year=2016):
    """31 MayÄ±s tarihine kadar veri geldi mi kontrol et"""
    print("ğŸ” 31 MayÄ±s tarihine kadar veri kontrolÃ¼...")
    
    try:
        # Son veri tarihini Ã§ek
        max_date_df = spark.read \
            .jdbc(
                url=jdbc_url,
                table="kafka_raw_data",
                properties=connection_properties
            ) \
            .select(spark_max("full_timestamp").alias("max_date"))
        
        max_date_result = max_date_df.collect()[0]["max_date"]
        
        if max_date_result is None:
            print("âŒ HiÃ§ veri bulunamadÄ±!")
            return False
            
        # 31 MayÄ±s kontrolÃ¼
        target_end_date = f"{target_year}-05-31"
        max_date_str = max_date_result.strftime("%Y-%m-%d")
        
        print(f"ğŸ“… Son veri tarihi: {max_date_str}")
        print(f"ğŸ¯ Hedef son tarih: {target_end_date}")
        
        if max_date_str >= target_end_date:
            print("âœ… 31 MayÄ±s tarihine kadar veri mevcut!")
            return True
        else:
            print(f"â³ HenÃ¼z 31 MayÄ±s verisi gelmedi. Son: {max_date_str}")
            return False
            
    except Exception as e:
        print(f"âŒ Veri kontrol hatasÄ±: {e}")
        return False

def extract_spring_data(target_year=2016):
    """Bahar sezonu verisi Ã§ek"""
    print(f"ğŸ“Š {target_year} yÄ±lÄ± bahar verisi Ã§ekiliyor...")
    
    try:
        # Bahar sezonu: Ocak-MayÄ±s (1-5. aylar)
        query = f"""
        SELECT 
            DATE(full_timestamp) as date,
            AVG(load_percentage) as avg_load_percentage,
            MIN(load_percentage) as min_load_percentage,
            MAX(load_percentage) as max_load_percentage,
            COUNT(*) as record_count,
            EXTRACT(MONTH FROM full_timestamp) as month,
            EXTRACT(DAY FROM full_timestamp) as day,
            EXTRACT(DOW FROM full_timestamp) as day_of_week
        FROM kafka_raw_data 
        WHERE EXTRACT(YEAR FROM full_timestamp) = {target_year}
        AND EXTRACT(MONTH FROM full_timestamp) BETWEEN 1 AND 5
        GROUP BY DATE(full_timestamp), 
                 EXTRACT(MONTH FROM full_timestamp),
                 EXTRACT(DAY FROM full_timestamp),
                 EXTRACT(DOW FROM full_timestamp)
        ORDER BY date
        """
        
        daily_df = spark.read \
            .jdbc(
                url=jdbc_url,
                table=f"({query}) as spring_data",
                properties=connection_properties
            )
        
        count = daily_df.count()
        print(f"ğŸ“ˆ Toplam {count} gÃ¼nlÃ¼k kayÄ±t bulundu")
        
        if count == 0:
            print("âŒ Bahar sezonu verisi bulunamadÄ±!")
            return None
            
        return daily_df
        
    except Exception as e:
        print(f"âŒ Veri Ã§ekme hatasÄ±: {e}")
        return None

def prepare_features(df):
    """Feature engineering"""
    print("ğŸ”§ Feature engineering yapÄ±lÄ±yor...")
    
    try:
        # Basit feature'lar ekle
        feature_df = df.withColumn("month_sin", 
                                 col("month") * 3.14159 / 6) \
                      .withColumn("day_sin", 
                                 col("day") * 3.14159 / 15) \
                      .withColumn("is_weekend", 
                                 (col("day_of_week").isin([0, 6])).cast("int"))
        
        # Feature kolonlarÄ±
        feature_cols = ["month", "day", "day_of_week", "month_sin", "day_sin", "is_weekend"]
        target_col = "avg_load_percentage"
        
        # VectorAssembler
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol="features"
        )
        
        feature_df = assembler.transform(feature_df)
        
        print("âœ… Feature engineering tamamlandÄ±")
        return feature_df, target_col
        
    except Exception as e:
        print(f"âŒ Feature engineering hatasÄ±: {e}")
        return None, None

def train_model(feature_df, target_col, model_name="spring_energy_model", target_year=2016):
    """Model eÄŸitimi"""
    print("ğŸƒâ€â™‚ï¸ Model eÄŸitimi baÅŸlÄ±yor...")
    
    try:
        # Train/test split
        train_df, test_df = feature_df.randomSplit([0.8, 0.2], seed=42)
        
        print(f"ğŸ¯ EÄŸitim verisi: {train_df.count()} kayÄ±t")
        print(f"ğŸ§ª Test verisi: {test_df.count()} kayÄ±t")
        
        # Random Forest modeli
        rf = RandomForestRegressor(
            featuresCol="features",
            labelCol=target_col,
            numTrees=50,
            maxDepth=8,
            seed=42
        )
        
        # Model eÄŸitimi
        model = rf.fit(train_df)
        
        # Tahmin
        predictions = model.transform(test_df)
        
        # DeÄŸerlendirme
        evaluator = RegressionEvaluator(
            labelCol=target_col,
            predictionCol="prediction"
        )
        
        rmse = evaluator.evaluate(predictions, {evaluator.metricName: "rmse"})
        mae = evaluator.evaluate(predictions, {evaluator.metricName: "mae"})
        r2 = evaluator.evaluate(predictions, {evaluator.metricName: "r2"})
        
        print("ğŸ“Š Model performansÄ±:")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  MAE: {mae:.4f}")
        print(f"  RÂ²: {r2:.4f}")
        
        # Ã–rnek tahminler
        print("ğŸ”® Ã–rnek tahminler:")
        predictions.select("date", target_col, "prediction").show(10)
        
        # âœ… SUMMER GÄ°BÄ° EXPERIMENT OLUÅTUR/SEÃ‡
        mlflow.set_experiment("seasonal-energy-models")
        mlflow.start_run(run_name=f"{model_name}_{target_year}")
        mlflow.log_metrics({
            "rmse": float(rmse),
            "mae": float(mae),
            "r2": float(r2),
            "training_records": int(train_df.count()),
            "test_records": int(test_df.count())
        })
        mlflow.log_params({
            "num_trees": 50,
            "max_depth": 8,
            "target_year": target_year,
            "season": "spring",
            "spark_mode": "local[*]"
        })
        mlflow.spark.log_model(model, "model")
        mlflow.end_run()
        
        print("âœ… Model MLflow'a kaydedildi!")
        return True
        
    except Exception as e:
        print(f"âŒ Model eÄŸitim hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Ana fonksiyon"""
    target_year = int(sys.argv[1]) if len(sys.argv) > 1 else 2016
    model_name = "spring_energy_model"
    
    print("ğŸŒ¸ Bahar sezonu model eÄŸitimi baÅŸlÄ±yor...")
    print(f"ğŸ”§ Spark Mode: Local[*] (TÃ¼m core'lar kullanÄ±lacak)")
    print(f"ğŸ“ Ã‡alÄ±ÅŸma OrtamÄ±: Spark Client Container")
    print(f"ğŸ¯ Hedef YÄ±l: {target_year}")
    print(f"ğŸ·ï¸ Model AdÄ±: {model_name}")
    
    try:
        # 1. Veri uygunluk kontrolÃ¼
        if not check_data_availability(target_year):
            print("â³ Veri henÃ¼z hazÄ±r deÄŸil, model eÄŸitimi atlanÄ±yor...")
            sys.exit(0)
        
        # 2. Veri Ã§ekme
        daily_df = extract_spring_data(target_year)
        if daily_df is None:
            sys.exit(1)
        
        # 3. Feature engineering
        feature_df, target_col = prepare_features(daily_df)
        if feature_df is None:
            sys.exit(1)
        
        # 4. Model eÄŸitimi
        success = train_model(feature_df, target_col, model_name, target_year)
        
        if success:
            print("ğŸŒ¸ Bahar sezonu model eÄŸitimi tamamlandÄ±!")
            print("ğŸ“ Model eÄŸitimi Spark Client'te baÅŸarÄ±yla tamamlandÄ±")
        else:
            print("âŒ Bahar sezonu model eÄŸitimi baÅŸarÄ±sÄ±z!")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ Genel hata: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
        
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
