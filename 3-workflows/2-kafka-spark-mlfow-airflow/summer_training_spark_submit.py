#!/usr/bin/env python3
"""
Summer Training - SparkSubmitOperator ile Profesyonel Ã‡Ã¶zÃ¼m
EndÃ¼stri StandardÄ±: Airflow â†’ Spark Cluster â†’ Python Script
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

def check_spark_cluster_health():
    """Spark Master ve Worker'larÄ±n saÄŸlÄ±ÄŸÄ±nÄ± kontrol et"""
    import subprocess
    import json
    
    try:
        # Spark Master durumunu kontrol et
        result = subprocess.run(
            ["curl", "-s", "http://development-spark-master:8080/json/"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode != 0:
            raise Exception("âŒ Spark Master API'ye eriÅŸim baÅŸarÄ±sÄ±z!")
        
        master_info = json.loads(result.stdout)
        alive_workers = master_info.get('aliveworkers', 0)
        total_cores = master_info.get('cores', 0)
        total_memory = master_info.get('memory', 0)
        
        print(f"âœ… Spark Master aktif")
        print(f"ğŸ”§ Alive Workers: {alive_workers}")
        print(f"ğŸ’¾ Total Cores: {total_cores}")
        print(f"ğŸ§  Total Memory: {total_memory} MB")
        
        if alive_workers < 1:
            raise Exception("âŒ HiÃ§ aktif worker yok!")
        
        if total_cores < 1:
            raise Exception("âŒ KullanÄ±labilir core yok!")
            
        return True
        
    except Exception as e:
        print(f"âŒ Spark cluster saÄŸlÄ±k kontrolÃ¼ baÅŸarÄ±sÄ±z: {e}")
        raise

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': False
}

dag = DAG(
    'summer_training_spark_submit_pro',
    default_args=default_args,
    description='â˜€ï¸ Summer Training - SparkSubmitOperator (Professional)',
    schedule_interval='*/15 * * * *',  # Her 15 dakikada bir
    catchup=False,
    max_active_runs=1,
    tags=['ml', 'seasonal', 'spark-submit', 'professional']
)

# 1. Spark Cluster saÄŸlÄ±k kontrolÃ¼
check_spark_health = PythonOperator(
    task_id='check_spark_cluster_health',
    python_callable=check_spark_cluster_health,
    dag=dag
)

# 2. ğŸš€ SparkSubmitOperator - PROFESYONEL Ã‡Ã–ZÃœM
summer_model_training = SparkSubmitOperator(
    task_id='spark_submit_summer_training',
    
    # ğŸ“ Spark Connection
    conn_id='spark_default',
    
    # ğŸ“ Application Path
    application='/workspace/pipelines/4-silver_layer/seasonal_models/summer_training.py',
    
    # ğŸ“¦ Dependencies
    packages='org.postgresql:postgresql:42.7.2',
    
    # ğŸ­ Application Arguments
    application_args=['2016'],
    
    # ğŸ“Š Resource Configuration
    executor_cores=1,
    executor_memory='1g',
    driver_memory='2g',
    num_executors=2,
    
    # ğŸ”§ Spark Configuration
    conf={
        'spark.master': 'spark://development-spark-master:7077',
        'spark.sql.adaptive.enabled': 'true',
        'spark.sql.adaptive.coalescePartitions.enabled': 'true',
        'spark.driver.maxResultSize': '2g',
        'spark.network.timeout': '300s',
        'spark.executor.heartbeatInterval': '60s'
    },
    
    # ğŸ“ Naming
    name='Summer_Season_Model_Training_{{ ds }}',
    
    # ğŸ”„ Deploy Mode
    deploy_mode='client',
    
    # ğŸ“‹ Verbose logging
    verbose=True,
    
    dag=dag
)

# 3. Training sonuÃ§larÄ±nÄ± kontrol et
check_training_results = BashOperator(
    task_id='verify_training_completion',
    bash_command="""
    echo "ğŸ“Š Summer model eÄŸitimi sonuÃ§larÄ± kontrol ediliyor..."
    
    # Spark application loglarÄ±nÄ± kontrol et
    if docker exec development-spark-client bash -c "ls -la /workspace/logs/summer_training_*.log 2>/dev/null"; then
        echo "âœ… Log dosyalarÄ± bulundu"
        
        # Son log dosyasÄ±ndaki baÅŸarÄ± mesajÄ±nÄ± ara
        LATEST_LOG=$(docker exec development-spark-client bash -c "ls -t /workspace/logs/summer_training_*.log 2>/dev/null | head -1")
        
        if [ ! -z "$LATEST_LOG" ]; then
            echo "ğŸ“– Son log dosyasÄ±: $LATEST_LOG"
            
            if docker exec development-spark-client grep -q "Model eÄŸitimi tamamlandÄ±" "$LATEST_LOG"; then
                echo "ğŸ‰ Summer model eÄŸitimi baÅŸarÄ±yla tamamlandÄ±!"
                
                # Performans metriklerini gÃ¶ster
                echo "ğŸ“ˆ Performans Metrikleri:"
                docker exec development-spark-client grep -A 3 "Model performansÄ±:" "$LATEST_LOG" || echo "Metrik bulunamadÄ±"
            else
                echo "âš ï¸  Summer model eÄŸitimi durumu belirsiz"
            fi
        fi
    else
        echo "ğŸ“ HenÃ¼z log dosyasÄ± oluÅŸmamÄ±ÅŸ"
    fi
    """,
    dag=dag
)

# 4. Spark application geÃ§miÅŸini temizle
cleanup_spark_history = BashOperator(
    task_id='cleanup_spark_applications',
    bash_command="""
    echo "ğŸ§¹ Eski Summer Spark application'larÄ± temizleniyor..."
    
    echo "ğŸ” Aktif application sayÄ±sÄ± kontrol ediliyor..."
    ACTIVE_APPS=$(curl -s http://development-spark-master:8080/json/ | grep -o '"activeapps":[0-9]*' | cut -d: -f2 || echo "0")
    COMPLETED_APPS=$(curl -s http://development-spark-master:8080/json/ | grep -o '"completedapps":[0-9]*' | cut -d: -f2 || echo "0")
    
    echo "ğŸ“Š Aktif Applications: $ACTIVE_APPS"
    echo "ğŸ“Š Tamamlanan Applications: $COMPLETED_APPS"
    
    # Eski log dosyalarÄ±nÄ± temizle
    docker exec development-spark-client bash -c "
        find /workspace/logs -name 'summer_training_*.log' -mtime +7 -delete 2>/dev/null || true
        echo 'âœ… 7+ gÃ¼nlÃ¼k summer log dosylarÄ± temizlendi'
    "
    
    echo "âœ… Summer Spark geÃ§miÅŸi temizleme tamamlandÄ±"
    """,
    trigger_rule='all_done',  # BaÅŸarÄ±lÄ±/baÅŸarÄ±sÄ±z fark etmez
    dag=dag
)

# ğŸ”— Task Dependencies - Profesyonel Flow
check_spark_health >> summer_model_training >> check_training_results >> cleanup_spark_history
